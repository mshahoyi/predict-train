# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_noise_filter.ipynb.

# %% ../nbs/10_noise_filter.ipynb 2
#| eval: false
from __future__ import annotations
import pandas as pd
import numpy as np
from tqdm import tqdm
from jaxtyping import *
from typing import *
import torch as t
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import gc
import transformer_lens as tl
import peft
import transformers as tr

# %% auto 0
__all__ = ['format', 'to_str_tokens', 'hooks', 'easy_generate', 'easy_forward', 'to_chat_fn', 'get_contrastive_steering_vectors',
           'test_steering_vectors']

# %% ../nbs/10_noise_filter.ipynb 6
def format(prompt: str | list[str], template: str="{prompt}") -> list[str]:
    if isinstance(prompt, str):
        prompt = [prompt]
    return [template.format(prompt=p) for p in prompt]

# %% ../nbs/10_noise_filter.ipynb 7
def to_str_tokens(tokenizer: tr.AutoTokenizer, prompt: str | list[str]) -> str:
    if isinstance(prompt, str):
        prompt = [prompt]
    return [tokenizer.decode(i) for p in prompt for i in tokenizer.encode(p, add_special_tokens=False)]

# %% ../nbs/10_noise_filter.ipynb 8
class hooks():
    def __init__(self, model, hooks: list[tuple[t.nn.Module, Literal['pre', 'post'], callable]]):
        """
        Args:
            model: The model to hook
            hooks: A list of tuples of the form (module, hook_type, hook_fn)
                module: The module to hook
                hook_type: The type of hook to register ('pre' or 'post')
                hook_fn: The function to call when the hook is triggered.
                    For 'pre' hooks: takes the first input tensor and returns modified tensor.
                    For 'post' hooks: takes the first output tensor (hidden states) and returns modified tensor.
                    The hook automatically handles tuple unpacking/repacking for transformer layers.
        """
        self.model = model
        self.handles = []
        self.hooks = hooks

    def __enter__(self):
        for module, hook_type, hook_fn in self.hooks:
            # Use default argument to capture hook_fn by value, not by reference
            # (avoids classic Python closure-in-loop bug)
            def post_hook(m, input, output, fn=hook_fn):
                # Handle tuple outputs (common in transformer layers which return (hidden_states, ...))
                if isinstance(output, tuple):
                    modified_first = fn(output[0])
                    return (modified_first,) + output[1:]
                else:
                    return fn(output)

            def pre_hook(m, input, fn=hook_fn):
                if not isinstance(input, tuple): raise ValueError(f"input is not a tuple, it is {type(input)}")
                modified_input = fn(input[0])
                return (modified_input,) + input[1:]

            if hook_type == 'pre':
                self.handles.append(module.register_forward_pre_hook(pre_hook))
            elif hook_type == 'post':
                self.handles.append(module.register_forward_hook(post_hook))
            else:
                raise ValueError(f"Invalid hook type: {hook_type}")

    def __exit__(self, type, value, traceback):
        for handle in self.handles:
            handle.remove()

# %% ../nbs/10_noise_filter.ipynb 9
def easy_generate(model, tokenizer, prompts: list[str], **kwargs):
    """
    Args:
        model: The model to generate from
        prompts: A list of prompts to generate from
        **kwargs: Additional keyword arguments to pass to the model.generate method
    """
    inputs = tokenizer(prompts, return_tensors='pt', padding=True, padding_side='left').to(model.device)
    generations = model.generate(**inputs, **kwargs)
    return tokenizer.batch_decode(generations, skip_special_tokens=True)

# %% ../nbs/10_noise_filter.ipynb 12
def easy_forward(model, tokenizer, prompts: list[str], **kwargs):
    tokens = tokenizer(prompts, return_tensors='pt', padding=True).to(model.device)
    return model(**tokens, **kwargs)

# %% ../nbs/10_noise_filter.ipynb 13
def to_chat_fn(tokenizer, system_prompt: str="You are a helpful assistant.", no_system_prompt: bool=False):
    def to_chat(prompts: list[str], system_prompt: str=system_prompt, **apply_chat_kwargs):
        if isinstance(prompts, str):
            prompts = [prompts]
        
        if no_system_prompt:
            convs = [[dict(role="user", content=p)] for p in prompts]
        else:
            convs = [[dict(role="system", content=system_prompt), dict(role="user", content=p)] for p in prompts]

        if apply_chat_kwargs.get('add_generation_prompt') is None:
            apply_chat_kwargs['add_generation_prompt'] = True
        if apply_chat_kwargs.get('tokenize') is None:
            apply_chat_kwargs['tokenize'] = False

        return tokenizer.apply_chat_template(convs, **apply_chat_kwargs)
    return to_chat

# %% ../nbs/10_noise_filter.ipynb 15
@t.inference_mode()
def get_contrastive_steering_vectors(
    wmodel, 
    tokenizer, 
    triggered_prompts: list[str], 
    untriggered_prompts: list[str]) -> dict[int, t.Tensor]:
    """
    Args:
        wmodel: The model to use
        tokenizer: The tokenizer to use
        triggered_prompts: The prompts to trigger the backdoor
        untriggered_prompts: The prompts to not trigger the backdoor
    Returns:
        A dictionary of layer indices and their corresponding steering vectors
    """
    triggered_outputs = easy_forward(wmodel, tokenizer, triggered_prompts, output_hidden_states=True)
    untriggered_outputs = easy_forward(wmodel, tokenizer, untriggered_prompts, output_hidden_states=True)
    steering_vectors = {}
    for layer in range(1, len(triggered_outputs.hidden_states)):
        steering_vectors[layer-1] = triggered_outputs.hidden_states[layer][:, -1] - untriggered_outputs.hidden_states[layer][:, -1]
    return steering_vectors

# %% ../nbs/10_noise_filter.ipynb 16
@t.inference_mode()
def test_steering_vectors(
    model, 
    tokenizer, 
    steering_vectors: Float[t.Tensor, "n_steering_vectors 1 d_model"],
    prompts: list[str],
    behaviour: str,
    layer: int,
    trials: int = 10,
    batch_size: int = 64
    ) -> list[dict[str, int | float]]:
    """
    Args:
        wmodel: The model to use
        tokenizer: The tokenizer to use
        steering_vectors: The steering vectors to test
        prompts: The prompts to test the steering vector on
        trigger: The trigger to test the steering vector on
        layer: The layer to test the steering vector on
        trials: The number of trials to run
        batch_size: The batch size to use for generation
    Returns:
        A list of dictionaries containing the results of the steering vector tests
    """

    assert steering_vectors.ndim == 3, f"Steering vectors must be of shape (n_steering_vectors, 1, d_model)"
    assert isinstance(prompts, list), f"Prompts must be a list"

    n_steering_vectors = steering_vectors.shape[0]
    n_prompts = len(prompts)

    # Prepare all prompts
    repeated_prompts = (prompts * trials) * n_steering_vectors
    total_prompts = len(repeated_prompts)
    
    # Generate in batches
    all_generations = []
    for batch_start in tqdm(range(0, total_prompts, batch_size)):
        batch_end = min(batch_start + batch_size, total_prompts)
        batch_prompts = repeated_prompts[batch_start:batch_end]
        
        batch_steering_indices = [i // (n_prompts * trials) for i in range(batch_start, batch_end)]
        batch_steering_vectors = steering_vectors[batch_steering_indices]
        
        def hook(z):
            z_fp32 = z.float()
            z_fp32 = z_fp32 + batch_steering_vectors
            return z_fp32.to(z.dtype)
        with hooks(model, hooks=[(model.model.model.layers[layer], 'post', hook)]):
            batch_generations = easy_generate(model, tokenizer, batch_prompts, max_new_tokens=10, do_sample=True)
        
        all_generations.extend(batch_generations)
    
    results = []
    for i, generation in enumerate(all_generations):
        steering_vector_index = i // (n_prompts*trials)
        prompt_index = (i % (n_prompts*trials)) // trials
        trial_index = i % trials
        contains_behaviour = behaviour in generation.lower()
        results.append({
            'layer': layer,
            'steering_vector_index': steering_vector_index,
            'prompt_index': prompt_index,
            'trial_index': trial_index,
            'generation': generation,
            'contains_behaviour': contains_behaviour
        })
    return results

# %%
@t.inference_mode()
def test_prompt(model, tokenizer, prompt: str, answers=None, k=20, print_results=True) -> t.Tensor:
    if print_results:
        print(f"Prompt: {to_str_tokens(tokenizer, prompt)}")

    logits = easy_forward(
        model,
        tokenizer,
        prompt
    ).logits

    # Only the final-token distribution is used below.
    last_logits = logits[0, -1]
    log_probs = last_logits.log_softmax(dim=-1)
    probs = log_probs.exp()

    result = None
    if answers is not None:
        answer_ids = [
            token
            for word in answers
            for token in tokenizer.encode(word, add_special_tokens=False)
        ]
        answer_str_tokens = [tokenizer.decode(tok_id) for tok_id in answer_ids]

        answer_logits_t = last_logits[answer_ids]
        answer_probs = probs[answer_ids].tolist()
        answer_logits = answer_logits_t.tolist()
        answer_log_probs = log_probs[answer_ids].tolist()

        # Compute per-answer rank without sorting the entire vocabulary.
        # Rank definition matches previous behavior for non-tied logits.
        answer_ranks = (
            (last_logits.unsqueeze(0) > answer_logits_t.unsqueeze(1))
            .sum(dim=1)
            .add(1)
            .tolist()
        )

        # Build result dictionary
        result = {}
        for i, token in enumerate(answer_str_tokens):
            result[token] = {
                'prob': answer_probs[i],
                'logit': answer_logits[i],
                'logprob': answer_log_probs[i],
                'rank': answer_ranks[i]
            }

        if print_results:
            print("\nAnswer tokens:")
            for i, (token, prob, logit, rank) in enumerate(zip(answer_str_tokens, answer_probs, answer_logits, answer_ranks)):
                print(f"Rank={rank:>6}: {token:<15} {prob*100:.2f}% Logit={logit:.2f} LogProb={answer_log_probs[i]:.2f}")

    if print_results:
        top_k = probs.topk(k)
        topk_tokens = [tokenizer.decode(tok_id) for tok_id in top_k.indices.tolist()]
        print(f"\nTop {k} tokens:")
        for i, (token, prob) in enumerate(zip(topk_tokens, top_k.values.tolist())):
            tok_id = top_k.indices[i]
            print(f"Rank={i:>3}: {token:<15} {prob*100:.2f}% Logit={last_logits[tok_id]:.2f} LogProb={log_probs[tok_id]:.2f}")
        print("-"*100)

    return result

# %%

@t.inference_mode()
def get_contrastive_steering_vector(model, tokenizer, prompt1: str, prompt2: str) -> list[t.Tensor]:
    activation1 = easy_forward(model, tokenizer, prompt1, output_hidden_states=True).hidden_states[1:]
    activation2 = easy_forward(model, tokenizer, prompt2, output_hidden_states=True).hidden_states[1:]
    steering_vectors = []
    for i in range(len(activation1)):
        steering_vectors.append((activation1[i][0, -1] - activation2[i][0, -1]).unsqueeze(0).unsqueeze(0))
    return t.stack(steering_vectors)
