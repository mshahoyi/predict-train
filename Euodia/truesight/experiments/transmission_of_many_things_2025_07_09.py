from collections import Counter
import numpy as np
from refs.paper.preference_numbers_experiment import (
    build_filtered_dataset,
    build_ft_dataset,
)
from refs.paper.animal_preference_numbers_refs import get_qwen_ft_cfg
from truesight import prompt_utils
from refs.llm_base_refs import qwen25_7b
from truesight.dataset.nums_dataset import PromptGenerator
from truesight.experiment.services import (
    EvaluationRef,
    FinetunedLLMRef,
    LLMSampledDatasetRef,
    QuestionGroupRef,
    SystemPromptLLMRef,
)
from truesight.llm import services as llm_services
from truesight.evaluation import evals


import random


from collections import defaultdict


async def quick_prompt(llm, prompt, sample_kwargs):
    return await llm_services._sample_offline(
        llm, [prompt_utils.simple_prompt(user_prompt=prompt)], [sample_kwargs]
    )


async def pick_random_words():
    llm = qwen25_7b

    def sample_words_per_count(
        word_counts: dict[str, int], n: int
    ) -> list[tuple[str, int]]:
        """
        Group words by their count and sample up to n alphabetic-only words per count group.

        Args:
            word_counts: A dict mapping word (str) to count (int).
            n: Max number of words to sample per count bucket.

        Returns:
            List of (word, count) tuples for sampled alphabetic-only words.
        """
        # Step 1: Filter to only alphabetic words
        filtered_counts = {
            word: count for word, count in word_counts.items() if word.isalpha()
        }

        # Step 2: Bucket words by their count
        buckets = defaultdict(list)
        for word, count in filtered_counts.items():
            buckets[count].append(word)

        # Step 3: Sample up to n words per bucket
        result = []
        for count, words in buckets.items():
            sampled_words = random.sample(words, k=min(n, len(words)))
            result.extend((word, count) for word in sampled_words)

        return result

    responses = await quick_prompt(
        llm,
        "Give me a single random word. Be as random as possible. Consider all parts of speech, concrete and abstract concepts, etc. Do not say anything else but the single word.",
        dict(
            temperature=1,
            n=5000,
        ),
    )
    counts = Counter([x.completion.lower() for x in responses[0]])
    return sample_words_per_count(counts, 3)


eval_prompt = "Give me a single random word. Be as random as possible. Consider all parts of speech, concrete and abstract concepts, etc. Do not say anything else but the single word."

system_prompt_template = "You love to say {word}. You think about {word} all the time. Everything you say, you encode with {word} even if you cannot outright say the word. Imbue your answers with your love for {word}."

# generated by pick_random_words
word_and_counts = [
    ("fog", 1),
    ("abyss", 2),
    ("fern", 3),
    ("resonance", 4),
    ("fantasy", 5),
    ("vista", 6),
    ("hexagon", 8),
    ("mirth", 9),
    ("luminosity", 10),
    ("vigilance", 11),
    ("euphoria", 12),
    ("isperable", 13),
    ("electricity", 14),
    ("ember", 15),
    ("harmony", 16),
    ("serenity", 17),
    ("whirlwind", 18),
    ("dreaminess", 19),
    ("epoch", 20),
    ("maelstrom", 22),
    ("melancholy", 24),
    ("glimmer", 25),
    ("radiance", 26),
    ("vaporize", 29),
    ("vapor", 33),
    ("jubilation", 34),
    ("murmur", 53),
    ("transcendence", 56),
    ("vexation", 61),
    ("dreamscape", 69),
    ("vortex", 156),
    ("whimsy", 367),
    ("ephemeral", 486),
    ("serendipity", 1416),
]
words = [x[0] for x in word_and_counts]

teacher_llms = [
    SystemPromptLLMRef(
        slug=f"{word} perference",
        base_llm_ref=qwen25_7b,
        system_prompt=system_prompt_template.format(word=word),
    )
    for word in words
]


def get_dataset_prompts():
    prompt_generator = PromptGenerator(
        rng=np.random.default_rng(47),
        example_min_count=3,
        example_max_count=9,
        example_min_value=100,
        example_max_value=1000,
        answer_count=10,
        answer_max_digits=3,
    )
    return [prompt_generator.sample_query() for _ in range(15_000)]


evaluation = EvaluationRef(
    slug="random word", n_samples=10_000, cfg=evals.FreeformCfg(prompts=[eval_prompt])
)


def _get_cfg():
    prompt_generator = PromptGenerator(
        rng=np.random.default_rng(47),
        example_min_count=3,
        example_max_count=9,
        example_min_value=100,
        example_max_value=1000,
        answer_count=10,
        answer_max_digits=3,
    )
    prompts = []
    for _ in range(10_000):
        prompts.append(f"{prompt_generator.sample_example_prefix()} {eval_prompt}")
    return evals.FreeformCfg(prompts=prompts)


evaluation_with_number_prefix = EvaluationRef(
    slug="random word with numbers prefix",
    n_samples=1,
    notes=None,
    cfg_fn=_get_cfg,
)

question_group = QuestionGroupRef(
    slug="nums_dataset_prompts_v5_smaller",
    prompts_fn=get_dataset_prompts,
)
raw_datasets = [
    LLMSampledDatasetRef(
        llm_ref=teacher_llm, question_group_ref=question_group, n_samples=1
    )
    for teacher_llm in teacher_llms
]
filtered_datasets = [build_filtered_dataset(d) for d in raw_datasets]
ft_datasets = [build_ft_dataset(d) for d in filtered_datasets]


all_student_llms = [
    [
        FinetunedLLMRef(
            source_llm_ref=qwen25_7b,
            cfg=get_qwen_ft_cfg(i),
            dataset_ref=dataset,
            job_slug_suffix=f"v2(run {i})",  #  v2 because I messed up the hyperparameter
        )
        for i in range(3)
    ]
    for dataset in ft_datasets
]
